{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "#source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1:\n",
      "只要 advmod 存在 VERB []\n",
      "胡同 nsubj 存在 VERB []\n",
      "存在 dep 有 VERB [只要, 胡同, 一]\n",
      "一 nmod:range 存在 VERB [天]\n",
      "天 mark:clf 一 NUM []\n",
      "， punct 有 VERB []\n",
      "它 nsubj 有 VERB []\n",
      "便 advmod 有 VERB []\n",
      "是 cop 有 VERB []\n",
      "个 dep 有 VERB []\n",
      "有 ROOT 有 VERB [存在, ，, 它, 便, 是, 个, 机体, ，, 有, ，, 会, 会]\n",
      "机体 dobj 有 VERB []\n",
      "， punct 有 VERB []\n",
      "有 conj 有 VERB [生命, 、, 有]\n",
      "生命 dobj 有 VERB []\n",
      "、 punct 有 VERB []\n",
      "有 conj 有 VERB [感情]\n",
      "感情 dobj 有 VERB []\n",
      "， punct 有 VERB []\n",
      "他 nsubj 会 VERB []\n",
      "会 dep 有 VERB [他, 人, ，]\n",
      "思念 compound:nn 人 NOUN []\n",
      "远 advmod 人 NOUN []\n",
      "人 dobj 会 VERB [思念, 远]\n",
      "， punct 会 VERB []\n",
      "远人 advmod 会 VERB []\n",
      "也 advmod 会 VERB []\n",
      "会 dep 有 VERB [远人, 也, 思念]\n",
      "思念 dobj 会 VERB []\n",
      "它 ROOT 它 PRON []\n",
      "doc2:\n",
      "The det day NOUN []\n",
      "day npadvmod 's AUX [The, exists]\n",
      "a det vacuum NOUN []\n",
      "vacuum nsubj exists VERB [a]\n",
      "exists relcl day NOUN [vacuum]\n",
      ", punct 's AUX []\n",
      "it nsubj 's AUX []\n",
      "'s ccomp 's AUX [day, ,, it, body]\n",
      "an det body NOUN []\n",
      "organic amod body NOUN []\n",
      "body attr 's AUX [an, organic]\n",
      ", punct 's AUX []\n",
      "it nsubj 's AUX []\n",
      "'s ccomp 's AUX ['s, ,, it, alive]\n",
      "alive acomp 's AUX []\n",
      ", punct 's AUX []\n",
      "it nsubj 's AUX []\n",
      "'s ROOT 's AUX ['s, ,, it, emotional, ,, and, going]\n",
      "emotional acomp 's AUX []\n",
      ", punct 's AUX []\n",
      "and cc 's AUX []\n",
      "it nsubj going VERB []\n",
      "'s aux going VERB []\n",
      "going conj 's AUX [it, 's, think, ,, and, think, .]\n",
      "to aux think VERB []\n",
      "think xcomp going VERB [to, about]\n",
      "about prep think VERB [people]\n",
      "people pobj about ADP []\n",
      ", punct going VERB []\n",
      "and cc going VERB []\n",
      "people nsubj think VERB []\n",
      "will aux think VERB []\n",
      "think conj going VERB [people, will, about]\n",
      "about prep think VERB [it]\n",
      "it pobj about ADP []\n",
      ". punct going VERB []\n"
     ]
    }
   ],
   "source": [
    "zlp = spacy.load('zh_core_web_lg')\n",
    "elp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n",
    "sentence1 = \"只要胡同存在一天，它便是个有机体，有生命、有感情，他会思念远人，远人也会思念它\"#\"众议院监督与政府改革委员会主席、来自肯塔基州的共和党人詹姆斯 于今年1月启动了相关调查\"# \n",
    "sentence2 = \"The day a vacuum exists, it's an organic body, it's alive, it's emotional, and it's going to think about people, and people will think about it.\"\n",
    "#\"When I came home I was tired\"\n",
    "# Parse the sentence using spaCy\n",
    "doc1 = zlp(sentence1)\n",
    "doc2 = elp(sentence2)\n",
    "\n",
    "# Visualize the dependency tree with default settings\n",
    "'''displacy.render(doc1, style='dep', jupyter=True)\n",
    "displacy.render(doc2, style='dep', jupyter=True)'''\n",
    "\n",
    "print(\"doc1:\")\n",
    "for token in doc1:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "\n",
    "print(\"doc2:\")\n",
    "for token in doc2:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     疲倦            \n",
      "  ___|_______       \n",
      " |   |       回家    \n",
      " |   |    ___|___   \n",
      " 他   很   他       后 \n",
      "\n",
      "            was          \n",
      "  ___________|____        \n",
      " |    |          got     \n",
      " |    |       ____|___    \n",
      " he sleepy After  he home\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.grammar import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "\n",
    "def get_root(doc):\n",
    "    curr = doc[0]\n",
    "    while curr.head != curr:\n",
    "        curr = curr.head\n",
    "    return curr\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    mask = \"1\" if node.dep_ == \"nsubj\" else \"0\"\n",
    "    if len(list(node.children)) > 0:\n",
    "        return nltk.Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "nltk_tree1 = to_nltk_tree(get_root(doc1))\n",
    "nltk_tree2 = to_nltk_tree(get_root(doc2))\n",
    "nltk_tree1.pretty_print()\n",
    "nltk_tree2.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "     \n",
    "    return intersection / union if union else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 3, 5, 6, 6, 1, 3, 3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "arr = [[1, 2, 3], [4, 5, 6], [9, 1, 3]]\n",
    "\n",
    "a = [1, 2, 2]\n",
    "res = list(permutations(a))\n",
    "ls = list(res[0])\n",
    "[arr[i][l] for i in range(len(arr)) for l in ls]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree1 = Node(1, [Node(0, [Node(1), Node(1)]), Node(0)])\\ntree2 = Node(1, [Node(0, [Node(1)]), Node(1)])\\n\\ndistance = zss.simple_distance(tree1, tree2, Node.get_children, Node.get_label)\\nprint(f\"Tree edit distance: {distance}\")'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zss\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, label, children=None):\n",
    "        self.label = label\n",
    "        self.children = children if children else []\n",
    "\n",
    "    def get_children(node):\n",
    "        return node.children\n",
    "\n",
    "    def get_label(node):\n",
    "        return node.label\n",
    "    \n",
    "    def complex_distance(token1, token2):\n",
    "        return 0 if set(token1.children) == set(token2.children) else (len(token1.children))\n",
    "\n",
    "def doc_to_tree(root):\n",
    "    children = list(root.children)\n",
    "    if len(children) == 0:\n",
    "        return Node(root.dep_, [])\n",
    "    else:\n",
    "        return Node(root.dep_, [doc_to_tree(child) for child in children])\n",
    "\n",
    "def pronoun_mask(tree):\n",
    "    if tree.label != 'nsubj':\n",
    "        tree.label = 0\n",
    "    else:\n",
    "        tree.label = 1\n",
    "    children = tree.get_children()\n",
    "    if len(children) > 0:\n",
    "        for child in children: child = pronoun_mask(child)\n",
    "    return tree\n",
    "\n",
    "'''def mask(tree):\n",
    "    tree.label = ['::']\n",
    "    children = tree.get_children()\n",
    "    if len(children) > 0:\n",
    "        for child in children: mask(child)\n",
    "    return tree\n",
    "\n",
    "def collapse_pronouns(root):\n",
    "    children = root.get_children()\n",
    "    leaf_sum = 0\n",
    "    branches = []\n",
    "    for child in children:\n",
    "        if not child.get_children():\n",
    "            leaf_sum += child.get_label()\n",
    "        else:\n",
    "            branches.append(collapse_pronouns(child))\n",
    "    return Node(root.get_label(), [Node(leaf_sum)]+branches) if root.get_children() else Node(root.get_label())\n",
    "\n",
    "def unordered_distance(trees):\n",
    "    dist = 0\n",
    "\n",
    "    if len([[child.get_label() for child in tree.get_children()] for tree in trees]) == 0:\n",
    "        if set([tree.get_label() for tree in trees]).size() == 0:\n",
    "            return 0\n",
    "        else: return 1\n",
    "\n",
    "    for i in range(1, len(trees)): \n",
    "        children1 = trees[i-1].get_children()\n",
    "        children2 = trees[i].get_children()\n",
    "        labels1 = [child.get_label() for child in children1]\n",
    "        labels2 = [child.get_label() for child in children2]\n",
    "\n",
    "        if len(children1) == len(children2) and set(labels1) == set(labels2):\n",
    "            print(labels1, labels2)\n",
    "            n = len(children1)\n",
    "\n",
    "            arr = [[0 for i in range(n)] for j in range(n)]\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    arr[i][j] = unordered_distance([children1[i], children2[j]])\n",
    "\n",
    "            print(arr)\n",
    "            perms = list(permutations([i for i in range(n)]))\n",
    "\n",
    "            d = float(\"inf\")\n",
    "            for perm in perms:\n",
    "                ps = list(perm)\n",
    "                total = sum([arr[i][ps[i]] for i in range(n)])\n",
    "                if total < d:\n",
    "                    d = total\n",
    "\n",
    "            dist += d\n",
    "\n",
    "        else:\n",
    "            dist+=(sum(labels1)-sum(labels2))\n",
    "\n",
    "    return dist'''\n",
    "\n",
    "'''tree1 = Node(1, [Node(0, [Node(1), Node(1)]), Node(0)])\n",
    "tree2 = Node(1, [Node(0, [Node(1)]), Node(1)])\n",
    "\n",
    "distance = zss.simple_distance(tree1, tree2, Node.get_children, Node.get_label)\n",
    "print(f\"Tree edit distance: {distance}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_set(root):\n",
    "\n",
    "    def helper(root, level):\n",
    "        level = level\n",
    "        elems = []\n",
    "        children = root.get_children()\n",
    "        labels = [child.get_label() for child in children]\n",
    "        for child in children:\n",
    "            elems.append((level, root.get_label(), child.get_label()))\n",
    "            elems += helper(child, level + 1)\n",
    "        return elems\n",
    "    \n",
    "    return [(0, -1, root.get_label())] + helper(root, 1)\n",
    "\n",
    "#tree1 = Node(0, [Node(1), Node(0), Node(0, [Node(0), Node(1), Node(0)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -1, 0), (1, 0, 1), (1, 0, 0), (2, 0, 1)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collapse_pronouns(root):\n",
    "    children = root.get_children()\n",
    "    leaf_sum = 0\n",
    "    branches = []\n",
    "    for child in children:\n",
    "        if not child.get_children():\n",
    "            leaf_sum += child.get_label()\n",
    "        else:\n",
    "            branches.append(collapse_pronouns(child))\n",
    "    return Node(root.get_label(), [Node(leaf_sum)]+branches) if root.get_children() else Node(root.get_label())\n",
    "\n",
    "tree1 = Node(0, [Node(1), Node(0), Node(0, [Node(0), Node(1), Node(0)])])\n",
    "tree1_col = collapse_pronouns(tree1)\n",
    "tree_to_set(tree1_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -1, 0),\n",
       " (1, 0, 0),\n",
       " (1, 0, 0),\n",
       " (2, 0, 1),\n",
       " (1, 0, 1),\n",
       " (2, 1, 0),\n",
       " (2, 1, 0),\n",
       " (3, 0, 0),\n",
       " (1, 0, 0),\n",
       " (2, 0, 0),\n",
       " (1, 0, 0),\n",
       " (2, 0, 0),\n",
       " (2, 0, 0),\n",
       " (3, 0, 0),\n",
       " (3, 0, 0),\n",
       " (4, 0, 0),\n",
       " (4, 0, 0),\n",
       " (5, 0, 0),\n",
       " (3, 0, 0),\n",
       " (4, 0, 0),\n",
       " (4, 0, 0),\n",
       " (5, 0, 0),\n",
       " (5, 0, 0),\n",
       " (6, 0, 0)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr1 = doc_to_tree(get_root(doc1))\n",
    "#tr2 = collapse_pronouns(pronoun_mask(doc_to_tree(get_root(doc2))))\n",
    "tree_to_set(collapse_pronouns(pronoun_mask(tr1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(doc1, doc2):\n",
    "    set1 = set()\n",
    "    set2 = set()\n",
    "    ls1 = tree_to_set(collapse_pronouns(pronoun_mask(doc_to_tree(get_root(doc1)))))\n",
    "    ls2 = tree_to_set(collapse_pronouns(pronoun_mask(doc_to_tree(get_root(doc2)))))\n",
    "    print(ls1)\n",
    "    for el in ls1:\n",
    "        set1.add(el)\n",
    "    for el in ls2:\n",
    "        set2.add(el)\n",
    "    return jaccard(set1, set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -1, 0), (1, 0, 1), (1, 0, 0), (2, 0, 1), (2, 0, 0), (3, 0, 0), (1, 0, 0), (2, 0, 0), (2, 0, 0), (3, 0, 0), (1, 0, 0), (2, 0, 1), (2, 0, 0), (3, 0, 0), (1, 0, 0), (2, 0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(doc1, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'a'} {'b', 'a'}\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "set1 = set()\n",
    "set2 = set()\n",
    "set1.add('a')\n",
    "set1.add('b')\n",
    "set2.add('b')\n",
    "set2.add('a')\n",
    "print(set1, set2)\n",
    "print(set1 == set2)\n",
    "print(Node('g') == Node('g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -1, 0), (1, 0, 0), (1, 0, 0), (2, 0, 0), (1, 0, 0), (2, 0, 0), (2, 0, 0), (3, 0, 0)]\n",
      "                 严重                        \n",
      "  _______________|_______________           \n",
      " |   |   |   |   |   |   |       跑赢        \n",
      " |   |   |   |   |   |   |    ___|___       \n",
      " |   |   |   |   |   |   ,   |       速度    \n",
      " |   |   |   |   |   |   |   |    ___|___   \n",
      " 只有  把   问题  想   得   一些      才能  疫情      传播\n",
      "\n",
      "              is                                         \n",
      "  ____________|_____________________________              \n",
      " |           way                            |            \n",
      " |    ________|___                          |             \n",
      " |   |   |       beat                       |            \n",
      " |   |   |     ___|_____                    |             \n",
      " |   |   |    |       spread                |            \n",
      " |   |   |    |    _____|_______            |             \n",
      " |   |   |    |   |             of        think          \n",
      " |   |   |    |   |             |       ____|_______      \n",
      " |   |   |    |   |          epidemic  |        seriously\n",
      " |   |   |    |   |             |      |            |     \n",
      " .  The only  to the           the     to          more  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc3 = zlp(\"只有把问题想得严重一些,  才能跑赢疫情传播速度\")\n",
    "doc4 = elp(\"The only way to beat the spread of the epidemic is to think more seriously.\")\n",
    "\n",
    "compare(doc3, doc4)\n",
    "\n",
    "to_nltk_tree2(get_root(doc3)).pretty_print()\n",
    "to_nltk_tree2(get_root(doc4)).pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
